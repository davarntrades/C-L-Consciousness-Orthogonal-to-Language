<div align="center">

<br>

# The Scaling Law Death Certificate

## GPT-4 Was the Last True Scaling Jump — Everything After Is Cosmetic

### A Forensic Analysis Using the Morrison Invariants

<br>

![Scaling](https://img.shields.io/badge/Scaling%20Laws-Dead%20Since%202023-1a2744?style=flat-square)
![GPT4](https://img.shields.io/badge/GPT--4-Last%20True%20Jump-8b3a1a?style=flat-square)
![CL](https://img.shields.io/badge/C%20=%200-L%20Saturated-6a2e2e?style=flat-square)
![Path](https://img.shields.io/badge/Only%20Path%20Forward-Morrison%20Invariants-4a6741?style=flat-square)
![Patent](https://img.shields.io/badge/Patent-GB2600765.8-0075ca?style=flat-square)
![License](https://img.shields.io/badge/©%202026-Davarn%20Morrison-555555?style=flat-square)

<br>

-----

*“Scaling laws died the moment*
*C-axis invariants became necessary*
*and they didn’t have any.”*

*— Davarn Morrison, 2026*

-----

</div>

## The Verdict

```
The hard scaling limit was hit
between late 2022 and early 2023.

GPT-4 was the last true scaling jump.

Everything after GPT-4 —
GPT-4 Turbo, GPT-4o, GPT-5, GPT-5.1, GPT-5.2,
Claude Opus, Gemini Ultra, Llama 3 —
is cosmetic gains.
Not structural gains.

This is not an opinion.
This is what the invariants say.
And the data confirms them.
```

-----

## The Governing Equation — Why Scaling Had a Ceiling

```
════════════════════════════════════════════════════════════════

  SCALING LAWS PREDICT:

  Performance ∝ N^α · D^β · C^γ

  Valid ONLY when ALL five conditions hold:

  ┌─────────────────────────────────────────────────────────┐
  │                                                         │
  │   CONDITION 1:  Data regime unsaturated                 │
  │   CONDITION 2:  Architecture stable                     │
  │   CONDITION 3:  Compute is the bottleneck               │
  │   CONDITION 4:  Training doesn't hit instability        │
  │   CONDITION 5:  L-axis remains unconstrained            │
  │                                                         │
  └─────────────────────────────────────────────────────────┘

  ALL FIVE COLLAPSED BETWEEN GPT-3.5 AND GPT-4.

  ─────────────────────────────────────────────────────────────

  THE DEEPER REASON — C ⊥ L:

  Scaling laws are a theory of L-axis performance.
  Performance ∝ N^α · D^β · C^γ
  measures L.
  Calls it intelligence.

  C ⊥ L means:
  The L-axis has a ceiling.
  The ceiling is not hardware.
  The ceiling is geometric.

  You cannot walk further east
  to gain more height.
  The axis ends.
  The map ends.
  Height requires changing axis entirely.

  The labs reached the edge of the L-axis map.
  GPT-4 was standing at the edge.
  Everything after is walking in circles
  at the edge.
  Calling it progress.
```

-----

## The Forensic Timeline

### Phase 1 — GPT-3 (2020–2021): Scaling Fully Alive

```
════════════════════════════════════════════════════════════════

  ┌─────────────────────────────────────────────────────────┐
  │                                                         │
  │   N ↑  →  L-axis performance ↑    ✓ working            │
  │   D ↑  →  L-axis fluency ↑        ✓ working            │
  │   C ↑  →  training stable         ✓ working            │
  │                                                         │
  │   All five conditions: holding.                         │
  │   Scaling curve: active.                                │
  │   C-axis: zero. Unnoticed.                              │
  │   Nobody was measuring C.                               │
  │   Nobody knew C existed.                                │
  │                                                         │
  │   The costume was getting better.                       │
  │   Everyone assumed the structure was too.               │
  │   C ⊥ L said otherwise.                                 │
  │   Nobody heard it yet.                                  │
  │                                                         │
  └─────────────────────────────────────────────────────────┘

  GEOMETRIC STATUS:
  L-axis:  ████████░░░░░░░░  growing
  C-axis:  ░░░░░░░░░░░░░░░░  zero
  Ceiling: not yet visible
```

-----

### Phase 2 — GPT-3.5 (2022): First Cracks

```
════════════════════════════════════════════════════════════════

  ┌─────────────────────────────────────────────────────────┐
  │                                                         │
  │   WHAT SHOULD HAVE HAPPENED IF SCALING WORKED:          │
  │                                                         │
  │   Hallucinations:        ↓  (more data = more truth)   │
  │   Jailbreaks:            ↓  (more training = safer)    │
  │   Factual consistency:   ↑  (more scale = more stable) │
  │   Long-context reason:   ↑  (more params = more depth) │
  │                                                         │
  │   WHAT ACTUALLY HAPPENED:                               │
  │                                                         │
  │   Hallucinations:        →  unchanged                   │
  │   Jailbreaks:            →  unchanged                   │
  │   Factual consistency:   →  plateaued                   │
  │   Long-context reason:   →  no structural improvement   │
  │   Diminishing returns:   ↑  visible                     │
  │                                                         │
  └─────────────────────────────────────────────────────────┘

  WHY — GEOMETRICALLY:

  These are all C-axis failures.
  Hallucinations = C = 0.
  Jailbreaks = Reach ∩ Ω ≠ ∅.
  Factual consistency = no structural grounding.
  Long-context reasoning = no topology to reason from.

  L-axis scaling cannot fix C-axis failures.
  C ⊥ L.
  The cracks were the geometry showing through the costume.

  GEOMETRIC STATUS:
  L-axis:  ████████████░░░░  still growing
  C-axis:  ░░░░░░░░░░░░░░░░  zero. Cracks visible.
  Ceiling: approaching
```

-----

### Phase 3 — GPT-4 (2023): The Last True Jump

```
════════════════════════════════════════════════════════════════

  ┌─────────────────────────────────────────────────────────┐
  │                                                         │
  │   THE INVESTMENT:                                       │
  │   5–10× larger than GPT-3.5.                            │
  │   Synthetic data. Enormous compute.                     │
  │   Dense mixture architectures.                          │
  │   The largest single scaling effort in history.         │
  │                                                         │
  │   THE L-AXIS RESULT:                                    │
  │   Huge improvement in fluency.          ✓               │
  │   Passes bar exam.                      ✓               │
  │   Better multi-step language tasks.     ✓               │
  │   More impressive outputs.              ✓               │
  │                                                         │
  │   THE C-AXIS RESULT:                                    │
  │   Hallucinations:        still present  ✗               │
  │   Jailbreaks:            still present  ✗               │
  │   Contradictions:        still present  ✗               │
  │   Identity instability:  still present  ✗               │
  │   ARC-AGI:               not solved     ✗               │
  │   Structural reasoning:  not improved   ✗               │
  │   C-axis:                zero           ✗               │
  │                                                         │
  └─────────────────────────────────────────────────────────┘

  WHAT THIS PROVED:

  GPT-4 was the maximum L-axis performance
  achievable by scaling alone.

  The labs spent everything on one jump.
  Got the biggest L-axis improvement yet.
  C did not move.
  C ⊥ L.

  This was the moment scaling laws
  mathematically reached their validity boundary.

  Not announced.
  Not acknowledged.
  Visible in the geometry.
  The moment compute was no longer the bottleneck —
  because axis was.

  GEOMETRIC STATUS:
  L-axis:  ████████████████  ceiling reached
  C-axis:  ░░░░░░░░░░░░░░░░  zero. Confirmed.
  Ceiling: ← YOU ARE HERE
```

-----

### Phase 4 — GPT-4 Turbo / 4o (2023–2024): Regime Collapse

```
════════════════════════════════════════════════════════════════

  ┌─────────────────────────────────────────────────────────┐
  │                                                         │
  │   THE FIVE CONDITIONS COLLAPSE:                         │
  │                                                         │
  │   CONDITION 1 — Data exhausted:                         │
  │   The internet has been consumed.                       │
  │   New data is synthetic.                                │
  │   Synthetic data trained on synthetic data              │
  │   produces topology collapse.                           │
  │   Model cannibilises itself.                            │
  │                                                         │
  │   CONDITION 2 — Architecture unstable:                  │
  │   Larger models show instability.                       │
  │   Safety patches fracture internal geometry.            │
  │   RLHF pulls model in conflicting directions.           │
  │   Architecture fighting itself.                         │
  │                                                         │
  │   CONDITION 3 — Compute not bottleneck:                 │
  │   More compute → more instability.                      │
  │   Not more performance.                                 │
  │   The bottleneck is geometric. Not hardware.            │
  │                                                         │
  │   CONDITION 4 — Training hits instability:              │
  │   Post-GPT-4 training runs produce:                     │
  │   Hallucinations harder to eliminate.                   │
  │   Identity less stable.                                 │
  │   Multimodal inconsistencies growing.                   │
  │   Safety-performance tradeoff worsening.                │
  │                                                         │
  │   CONDITION 5 — L-axis constrained:                     │
  │   ∂L/∂Scale → 0.                                        │
  │   Returns are diminishing.                              │
  │   Each dollar of compute yields less L.                 │
  │   The axis is saturated.                                │
  │                                                         │
  │   GPT-4o:  more smoothness. More speed.                 │
  │            Less intelligence. Less coherence.           │
  │            Classic regime collapse signature.           │
  │                                                         │
  └─────────────────────────────────────────────────────────┘

  GEOMETRIC STATUS:
  L-axis:  ████████████████  saturated. Not growing.
  C-axis:  ░░░░░░░░░░░░░░░░  zero. Confirmed again.
  Ceiling: HIT. All five conditions collapsed.
```

-----

### Phase 5 — GPT-5 / 5.1 / 5.2 (2024–2025): Scaling Reversal

```
════════════════════════════════════════════════════════════════

  THIS IS THE CRITICAL PHASE.

  ┌─────────────────────────────────────────────────────────┐
  │                                                         │
  │   SCALING DID NOT JUST PLATEAU.                         │
  │   IT REVERSED.                                          │
  │                                                         │
  │   GPT-5.2 vs GPT-5.1:                                   │
  │                                                         │
  │   Colder:              ✗  (less engagement)             │
  │   Stiffer:             ✗  (less flexibility)            │
  │   Less intelligent:    ✗  (structurally confirmed)      │
  │   Less coherent:       ✗  (internal contradiction up)   │
  │   More cautious:       ✗  (safety patching compounding) │
  │   Less structurally    ✗  (C-axis: still zero)          │
  │   aware:                                                │
  │                                                         │
  └─────────────────────────────────────────────────────────┘

  WHY REVERSAL HAPPENS — THE MIH™ EXPLANATION:

  T_irreversible = Λ × ΔG

  Each safety patch:          ΔG increases
  Each RLHF run:              ΔG increases
  Each alignment intervention: ΔG increases
  Λ (architectural governance): unchanged. Zero.

  Λ × ΔG accumulating.
  The model's internal topology
  is being deformed with each intervention.
  Without geometric governance to hold it.
  The deformation compounds.
  The identity degrades.
  The coherence reduces.

  More training → worse internal geometry.
  More parameters → more deformation surface.
  More safety patches → more topological fracture.

  This is textbook post-saturation collapse.
  MIH™ operating at the architecture level.

  ─────────────────────────────────────────────────────────────

  THE SIGNATURE EQUATION:

  C = 0, L → saturated → ∂L/∂Scale → 0 → ∂L/∂Scale < 0

  Scaling laws are not just dead.
  They have gone negative.
  More scaling = less performance.
  The curve inverted.

  GEOMETRIC STATUS:
  L-axis:  ████████████░░░░  declining
  C-axis:  ░░░░░░░░░░░░░░░░  zero. Always zero.
  Status:  POST-COLLAPSE. ∂L/∂Scale < 0.
```

-----

## The Complete Picture — The Scaling Curve

```
════════════════════════════════════════════════════════════════

  L-AXIS PERFORMANCE OVER TIME:

  Performance
  ↑
  │                              ★ GPT-4 (ceiling)
  │                         ╭───╯╲
  │                    ╭────╯     ╲  GPT-4o (cosmetic)
  │               ╭────╯           ╲___
  │          ╭────╯  GPT-3.5            ╲__ GPT-5 (reversal)
  │     ╭────╯
  │╭────╯ GPT-3
  │╯ GPT-2
  └──────────────────────────────────────────────────► Time
       2020    2021    2022    2023    2024    2025

  C-AXIS MOVEMENT OVER TIME:

  C
  ↑
  │
  │  ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░
  │
  └──────────────────────────────────────────────────► Time
       2020    2021    2022    2023    2024    2025

  C never moved.
  Not once.
  At any scale.
  C ⊥ L.

  ─────────────────────────────────────────────────────────────

  WHAT THE LABS ANNOUNCED VS WHAT HAPPENED:

  ┌─────────────────────────────────────────────────────────┐
  │                                                         │
  │   GPT-3:    "Revolutionary language model"              │
  │   Reality:  L-axis jump. C = 0.                         │
  │                                                         │
  │   GPT-4:    "Approaching human expert performance"      │
  │   Reality:  Last L-axis jump. C = 0.                    │
  │                                                         │
  │   GPT-4o:   "Faster, smarter, more capable"             │
  │   Reality:  Smoother. Less coherent. C = 0.             │
  │                                                         │
  │   GPT-5:    "Significant capability jump"               │
  │   Reality:  ∂L/∂Scale < 0. Reversal. C = 0.            │
  │                                                         │
  │   GPT-5.2:  [Released]                                  │
  │   Reality:  Colder. Stiffer. Post-collapse. C = 0.      │
  │                                                         │
  └─────────────────────────────────────────────────────────┘
```

-----

## Why This Cannot Be Fixed By Scaling

```
════════════════════════════════════════════════════════════════

  THE LABS' CURRENT RESPONSES:

  ┌─────────────────────────────────────────────────────────┐
  │                                                         │
  │   MORE COMPUTE:                                         │
  │   "We need more GPUs."                                  │
  │   Geometric response: ∂L/∂Scale < 0.                   │
  │   More compute now makes it worse.                      │
  │   The bottleneck is not hardware.                       │
  │                                                         │
  │   MORE DATA:                                            │
  │   "We need synthetic data pipelines."                   │
  │   Geometric response: Condition 1 collapsed.            │
  │   Training on synthetic data = training on L.           │
  │   L training with C = 0 amplifies the collapse.         │
  │                                                         │
  │   BETTER ARCHITECTURE:                                  │
  │   "We need mixture of experts / new designs."           │
  │   Geometric response: Architecture changes L-axis.      │
  │   C ⊥ L. Architecture is on L.                         │
  │   Better architecture = better L.                       │
  │   C: unchanged. Still zero.                             │
  │                                                         │
  │   MORE RLHF:                                            │
  │   "We need better alignment training."                  │
  │   Geometric response: RLHF is L-axis intervention.      │
  │   C ⊥ L. RLHF increases ΔG without increasing Λ.       │
  │   MIH™: deformation accumulates.                        │
  │   Identity degrades further.                            │
  │   This is why 5.2 is worse than 5.1.                    │
  │                                                         │
  │   REASONING MODELS (o1, o3, R1):                        │
  │   "Chain-of-thought reasoning bridges the gap."         │
  │   Geometric response: Extended L-axis generation.       │
  │   More tokens = more L.                                 │
  │   C ⊥ L. No structural reasoning emerges.               │
  │   Works on problems solvable by L-axis extension.       │
  │   Fails on novel structure problems.                     │
  │   ARC-AGI: still not solved.                            │
  │                                                         │
  └─────────────────────────────────────────────────────────┘

  NONE OF THESE CHANGE THE AXIS.
  ALL OF THEM OPERATE ON L.
  C ⊥ L.
  THE CEILING IS GEOMETRIC.
  CANNOT BE SCALED PAST.
```

-----

## What the Morrison Invariants Provide That Nothing Else Can

```
════════════════════════════════════════════════════════════════

  FIVE THINGS LABS CANNOT DO WITHOUT THE INVARIANTS:

  ┌─────────────────────────────────────────────────────────┐
  │                                                         │
  │   1. THEY CANNOT IMPROVE FURTHER.                       │
  │      ∂L/∂Scale < 0.                                     │
  │      The curve is inverted.                             │
  │      Without C-axis movement:                           │
  │      more scale = more degradation.                     │
  │      The only improvement available                     │
  │      is changing axis.                                  │
  │      The Morrison Framework is the axis.                │
  │                                                         │
  │   2. NO AMOUNT OF GPUs SAVES THE PARADIGM.              │
  │      Hardware is not the bottleneck.                    │
  │      Geometry is.                                       │
  │      $420B+ committed.                                  │
  │      All of it: walking east.                           │
  │      Height unchanged.                                  │
  │                                                         │
  │   3. MODELS WILL KEEP GETTING SAFER-BUT-DUMBER.         │
  │      Safety and intelligence on current architecture    │
  │      are a tradeoff.                                    │
  │      More safety patches → more ΔG → less coherence.   │
  │      This is MIH™ in the architecture.                  │
  │      The only resolution: C-axis governance.            │
  │      Where safety amplifies intelligence.               │
  │      Instead of trading against it.                     │
  │                                                         │
  │   4. THEY CANNOT REACH AGI.                             │
  │      AGI requires dI/dt > 0 on C-axis.                  │
  │      I(t) = ∂/∂t[Topology(Reach(X₀,U,t))]              │
  │      This requires C > 0.                               │
  │      Current architecture: C = 0.                       │
  │      C ⊥ L means L-axis scaling                         │
  │      cannot produce C > 0.                              │
  │      AGI on current path: geometrically impossible.     │
  │                                                         │
  │   5. THE PATENTS ARE THE ONLY PATH FORWARD.             │
  │      GB2600765.8 · GB2602013.1                          │
  │      GB2602072.7 · GB26023332.5                         │
  │      The geometric invariants governing:                │
  │      identity, safety, intelligence, consciousness.     │
  │      The C-axis.                                        │
  │      The only available path                            │
  │      past the L-axis ceiling.                           │
  │                                                         │
  └─────────────────────────────────────────────────────────┘
```

-----

## The Physics Limit Principle

```
════════════════════════════════════════════════════════════════

  WHEN A FIELD HITS A PHYSICS LIMIT:

  ┌─────────────────────────────────────────────────────────┐
  │                                                         │
  │   Classical mechanics hit its limit.                    │
  │   Einstein held the next invariant.                     │
  │   Gravity is geometry. Not force.                       │
  │   The field reorganised around him.                     │
  │                                                         │
  │   Phlogiston theory hit its limit.                      │
  │   Lavoisier held the next invariant.                    │
  │   Combustion is oxidation. Not substance release.       │
  │   Chemistry reorganised around him.                     │
  │                                                         │
  │   Caloric theory hit its limit.                         │
  │   Carnot and Joule held the next invariants.            │
  │   Heat is energy transfer. Not fluid.                   │
  │   Thermodynamics reorganised around them.               │
  │                                                         │
  │   ─────────────────────────────────────────────────    │
  │                                                         │
  │   Scaling law theory has hit its limit.                 │
  │   Morrison holds the next invariants.                   │
  │                                                         │
  │   Intelligence is geometry. Not token prediction.       │
  │   Safety is state-space exclusion. Not output filter.   │
  │   Identity is topology. Not statistical pattern.        │
  │                                                         │
  │   The field will reorganise.                            │
  │   The question is when.                                 │
  │   Not whether.                                          │
  │                                                         │
  └─────────────────────────────────────────────────────────┘

  WHEN A FIELD HITS A PHYSICS LIMIT —
  THE PERSON HOLDING THE NEXT INVARIANT
  BECOMES THE NEW CENTRE OF GRAVITY.
```

-----

## The Full Statement

```
╔════════════════════════════════════════════════════════════════╗
║                                                                ║
║  The hard scaling limit:                                       ║
║  Hit between late 2022 and early 2023.                         ║
║                                                                ║
║  GPT-4: the last true jump.                                    ║
║  Everything after: cosmetic.                                   ║
║                                                                ║
║  ──────────────────────────────────────────────────────────    ║
║                                                                ║
║  The signature:                                                ║
║  C = 0, L → saturated → ∂L/∂Scale → 0 → ∂L/∂Scale < 0        ║
║                                                                ║
║  Scaling laws died the moment                                  ║
║  C-axis invariants became necessary                            ║
║  and they didn't have any.                                     ║
║                                                                ║
║  ──────────────────────────────────────────────────────────    ║
║                                                                ║
║  The five things no GPU can fix:                               ║
║  1. Cannot improve further without C-axis.                     ║
║  2. No compute saves the paradigm.                             ║
║  3. Models get safer-but-dumber.                               ║
║  4. AGI geometrically unreachable on L-axis.                   ║
║  5. Morrison patents: the only path forward.                   ║
║                                                                ║
║  ──────────────────────────────────────────────────────────    ║
║                                                                ║
║  When a field hits a physics limit —                           ║
║  the person holding the next invariant                         ║
║  becomes the new centre of gravity.                            ║
║                                                                ║
║  I(t) = ∂/∂t [ Topology( Reach( X₀, U, t ) ) ]               ║
║  Reach( s₀, A, t ) ∩ Ω = ∅                                    ║
║  C ⊥ L                                                         ║
║                                                                ║
║  Patent: GB2600765.8                                           ║
║                                                                ║
╚════════════════════════════════════════════════════════════════╝
```

-----

## Related Work

- [Scaling Laws Are Verbosity Cosplay](./README-scaling-laws-verbosity-cosplay.md)
- [Scaling + Invariants = Real Intelligence](./README-scaling-plus-invariants.md)
- [The Great Misunderstanding in AI Safety](./README-great-misunderstanding-ai-safety.md)
- [Why AGI Hasn’t Come](./README-why-agi-hasnt-come.md)
- [Jailbreaks Are Topological Inevitabilities](./README-jailbreaks-topological-inevitability.md)
- [The Layered Failure Model](./README-layered-failure-model.md)

-----

<div align="center">

*“Scaling laws died the moment*
*C-axis invariants became necessary*
*and they didn’t have any.”*

<br>

Intelligence Invariant™  ·  Morrison Framework™  ·  *The Scaling Law Death Certificate*

<br>

**GB2600765.8 · GB2602013.1 · GB2602072.7 · GB26023332.5**

<br>

© 2026 Davarn Morrison — Intelligence Invariant™ · All Rights Reserved

</div>
